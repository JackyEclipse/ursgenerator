"""
Models for ingestion and processing pipeline.
"""

from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
from datetime import datetime
from enum import Enum


class SourceType(str, Enum):
    """Types of input sources."""
    DOCUMENT = "document"
    EMAIL = "email"
    MEETING_NOTES = "meeting_notes"
    INTERVIEW = "interview"
    SCREENSHOT = "screenshot"
    LINK = "link"
    USER_INPUT = "user_input"


class SourceChunk(BaseModel):
    """
    A chunk of source material with unique ID for traceability.
    Every piece of information is tracked back to its source.
    """
    chunk_id: str = Field(..., description="Unique identifier for this chunk")
    source_id: str = Field(..., description="ID of the parent source document")
    source_type: SourceType
    source_name: str = Field(..., description="Original filename or source identifier")
    
    # Content
    content: str = Field(..., description="The actual text content")
    content_hash: str = Field(..., description="SHA-256 hash for deduplication")
    
    # Position in source
    start_offset: Optional[int] = None
    end_offset: Optional[int] = None
    page_number: Optional[int] = None
    
    # Extracted entities (from Stage 1 normalization)
    entities: Dict[str, List[str]] = Field(
        default_factory=dict,
        description="Extracted entities: requirements, constraints, stakeholders, etc."
    )
    
    # Metadata
    created_at: datetime = Field(default_factory=datetime.utcnow)
    data_classification: str = "INTERNAL"
    
    # Processing status
    is_normalized: bool = False
    normalized_at: Optional[datetime] = None


class NormalizedFact(BaseModel):
    """
    A structured fact extracted from source chunks.
    Output of Stage 1 normalization.
    """
    fact_id: str
    fact_type: str = Field(
        ..., 
        description="requirement, constraint, stakeholder, context, pain_point, goal"
    )
    content: str
    source_chunk_ids: List[str] = Field(..., min_length=1)
    confidence: float = Field(..., ge=0.0, le=1.0)
    is_assumption: bool = False
    metadata: Dict[str, Any] = Field(default_factory=dict)


class IngestRequest(BaseModel):
    """Request to ingest new source material."""
    title: str = Field(..., description="Project/request title")
    description: Optional[str] = Field(None, description="Brief description of the request")
    requestor_name: str
    requestor_email: str
    department: str
    
    # Raw text inputs
    raw_text: Optional[str] = Field(None, description="Free-form text input")
    meeting_notes: Optional[str] = None
    email_thread: Optional[str] = None
    
    # File references (files uploaded separately via multipart)
    file_ids: List[str] = Field(default_factory=list)
    
    # Settings
    data_classification: str = "INTERNAL"


class IngestResponse(BaseModel):
    """Response from ingestion endpoint."""
    session_id: str = Field(..., description="Session ID for this ingestion")
    urs_id: str = Field(..., description="Generated URS ID")
    chunks_created: int
    normalized_facts: int
    
    # Preview of extracted content
    extracted_summary: Optional[str] = None
    key_entities: Dict[str, List[str]] = Field(default_factory=dict)
    
    # Next steps
    needs_clarification: bool
    clarifying_questions_count: int = 0


class ClarifyingQuestion(BaseModel):
    """
    A question generated by Stage 2 to resolve ambiguity.
    """
    question_id: str
    question: str
    context: str = Field(..., description="Why this question is being asked")
    related_chunk_ids: List[str] = Field(
        ..., 
        description="Source chunks that prompted this question"
    )
    category: str = Field(
        ..., 
        description="missing_info, contradiction, ambiguity, scope_unclear"
    )
    priority: str = Field(default="medium", description="high, medium, low")
    suggested_options: List[str] = Field(
        default_factory=list,
        description="Optional multiple-choice options"
    )


class ClarifyRequest(BaseModel):
    """Request to get clarifying questions for a session."""
    session_id: str
    urs_id: Optional[str] = None


class ClarifyResponse(BaseModel):
    """Response with clarifying questions."""
    session_id: str
    questions: List[ClarifyingQuestion]
    completeness_score: float = Field(
        ..., 
        ge=0.0, 
        le=1.0,
        description="How complete the information is (1.0 = no questions needed)"
    )


class AnswerSubmission(BaseModel):
    """User's answer to a clarifying question."""
    question_id: str
    answer: str
    additional_context: Optional[str] = None


class AnswersRequest(BaseModel):
    """Submit answers to clarifying questions."""
    session_id: str
    urs_id: Optional[str] = None
    answers: List[AnswerSubmission]


class GenerateRequest(BaseModel):
    """Request to generate URS from ingested content."""
    session_id: str
    urs_id: Optional[str] = None
    skip_clarification: bool = Field(
        default=False,
        description="Generate even if clarifying questions are pending"
    )


class GenerateResponse(BaseModel):
    """Response from URS generation."""
    urs_id: str
    status: str = Field(..., description="success, partial, failed")
    urs: Optional[Dict[str, Any]] = Field(None, description="Generated URS document")
    warnings: List[str] = Field(default_factory=list)
    assumptions_made: int = 0
    low_confidence_requirements: int = 0


class ReviewRequest(BaseModel):
    """Request for QA review of generated URS."""
    urs_id: str


class QAIssue(BaseModel):
    """An issue found during QA review."""
    issue_id: str
    severity: str = Field(..., description="critical, warning, suggestion")
    category: str = Field(
        ..., 
        description="vague_language, missing_acceptance_criteria, untestable, assumption, contradiction"
    )
    location: str = Field(..., description="Path in URS, e.g., 'functional_requirements[2].description'")
    description: str
    suggestion: Optional[str] = None
    affected_requirement_id: Optional[str] = None


class ReviewResponse(BaseModel):
    """Response from QA review."""
    urs_id: str
    overall_score: float = Field(..., ge=0.0, le=100.0)
    scores: Dict[str, float] = Field(
        ..., 
        description="Breakdown: completeness, clarity, testability, traceability"
    )
    issues: List[QAIssue]
    ready_for_approval: bool
    blocking_issues_count: int

